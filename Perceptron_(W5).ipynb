{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN54M/R8YpoBOXu/wD85BgA"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PA"
      ],
      "metadata": {
        "id": "8DHBRcbv3gPS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EGZw2nvCMb6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y=fetch_openml('mnist_784',version=1,return_X_y=True)\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1uh9KKrC94b",
        "outputId": "98a75bc2-805d-4fda-e88b-95ef17de20f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
            "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
            "69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
            "\n",
            "       pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
            "0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "...        ...  ...       ...       ...       ...       ...       ...   \n",
            "69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
            "\n",
            "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
            "0           0.0       0.0       0.0       0.0       0.0  \n",
            "1           0.0       0.0       0.0       0.0       0.0  \n",
            "2           0.0       0.0       0.0       0.0       0.0  \n",
            "3           0.0       0.0       0.0       0.0       0.0  \n",
            "4           0.0       0.0       0.0       0.0       0.0  \n",
            "...         ...       ...       ...       ...       ...  \n",
            "69995       0.0       0.0       0.0       0.0       0.0  \n",
            "69996       0.0       0.0       0.0       0.0       0.0  \n",
            "69997       0.0       0.0       0.0       0.0       0.0  \n",
            "69998       0.0       0.0       0.0       0.0       0.0  \n",
            "69999       0.0       0.0       0.0       0.0       0.0  \n",
            "\n",
            "[70000 rows x 784 columns]\n",
            "0        5\n",
            "1        0\n",
            "2        4\n",
            "3        1\n",
            "4        9\n",
            "        ..\n",
            "69995    2\n",
            "69996    3\n",
            "69997    4\n",
            "69998    5\n",
            "69999    6\n",
            "Name: class, Length: 70000, dtype: category\n",
            "Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF18wQvxD4B4",
        "outputId": "5551de3d-4521-4dc4-8cba-8877d09c1ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70000, 784)\n",
            "(70000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=X.to_numpy()\n",
        "y=y.to_numpy()"
      ],
      "metadata": {
        "id": "8-eBtYa0FpZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[2022])\n",
        "print(y[2022])\n",
        "print(X[2022].mean())\n",
        "unique_values, unique_counts = np.unique(X[2022], return_counts=True)\n",
        "print(unique_values, unique_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QACsO1H_En7j",
        "outputId": "d79d0521-23ac-4c2b-f3c6-01e81ac2ef2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  48.  18.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   3.  71. 149. 223. 242. 230.\n",
            " 223. 153.  29.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.  14. 185. 254. 254. 254. 254. 240.\n",
            " 251. 254. 254. 100.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.  64. 209. 254. 206. 138.  65.  65.  43.\n",
            "  60. 151. 254. 249.  65.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0. 240. 254. 254. 108.   0.   0.   0.   0.\n",
            "   0.   6. 155. 254. 137.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.  73. 205. 205. 170.   0.   0.   0.   0.\n",
            "   0.   0.  36. 245. 206.  10.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0. 239. 254.  48.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0. 239. 254.  48.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.  60. 249. 242.  38.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.  20. 198. 254. 109.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.  64. 254. 225.   6.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   2.  79. 133.  60.  60.  14.\n",
            "  84. 224. 254.  53.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   6. 254. 254. 254. 254. 218.\n",
            " 230. 254. 254. 108.  99.  99.  99.  99. 188. 207. 207. 207.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   6. 254. 176.  11.  79. 254.\n",
            " 254. 237. 227. 227. 227. 227. 253. 254. 141. 119.  89.  10.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   6. 254. 229. 131. 254. 254.\n",
            " 169.  29.   0.   0.   0.   0.  78.  81.  14.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   1. 169. 254. 254. 254. 175.\n",
            "  31.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.  87. 246. 253. 201.  27.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  57.  72.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "2\n",
            "27.761479591836736\n",
            "[  0.   1.   2.   3.   6.  10.  11.  14.  18.  20.  27.  29.  31.  36.\n",
            "  38.  43.  48.  53.  57.  60.  64.  65.  71.  72.  73.  78.  79.  81.\n",
            "  84.  87.  89.  99. 100. 108. 109. 119. 131. 133. 137. 138. 141. 149.\n",
            " 151. 153. 155. 169. 170. 175. 176. 185. 188. 198. 201. 205. 206. 207.\n",
            " 209. 218. 223. 224. 225. 227. 229. 230. 237. 239. 240. 242. 245. 246.\n",
            " 249. 251. 253. 254.] [643   2   1   1   5   2   1   3   1   1   1   2   1   1   1   1   3   1\n",
            "   1   4   2   3   1   1   1   1   2   1   1   1   1   4   1   2   1   1\n",
            "   1   1   1   1   1   1   1   1   1   2   1   1   1   1   1   1   1   2\n",
            "   2   3   1   1   2   1   1   4   1   2   1   2   2   2   1   1   2   1\n",
            "   2  32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X[:10000]\n",
        "y_train = y[:10000]\n",
        "X_test = X[10000:12000]\n",
        "y_test = y[10000:12000]"
      ],
      "metadata": {
        "id": "hzog9ScBKpvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(np.unique(y_train)))\n",
        "print(np.count_nonzero(y_train == '6'))\n",
        "print(np.count_nonzero(y_train == '9'))\n",
        "print(len(np.unique(y_test)))\n",
        "max=np.count_nonzero(y_train == '9')\n",
        "res=20\n",
        "for i in range(10):\n",
        "  print(np.count_nonzero(y_train == str(i)), end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7g4Yr9jO6T0",
        "outputId": "797a4c2b-26cc-49d7-aa94-61d9348d3501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "1014\n",
            "978\n",
            "10\n",
            "1001 1127 991 1032 980 863 1014 1070 944 978 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_6_indices = np.where(y_train=='6')\n",
        "X_9_indices = np.where(y_train=='9')"
      ],
      "metadata": {
        "id": "Hhp-rWKu4kAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_6_indices_test = np.where(y_test=='6')\n",
        "X_9_indices_test = np.where(y_test=='9')"
      ],
      "metadata": {
        "id": "7wyFkfLV_ROs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_positive = X_train[X_6_indices]\n",
        "x_negative = X_train[X_9_indices]\n",
        "print(x_positive.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm_z_AkcomNz",
        "outputId": "e8985a33-faa0-4b3f-80f9-22a34a13e3a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1014, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_positive_test = X_test[X_6_indices_test]\n",
        "x_negative_test = X_test[X_9_indices_test]\n",
        "print(x_positive_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo9VPb-6_b2D",
        "outputId": "04ff83af-3bd8-423d-916a-71891aabc6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(194, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = np.vstack((x_positive,x_negative))\n",
        "X_new"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVZKSfnPv80I",
        "outputId": "ed9c736c-25e4-4344-d4b6-42d825a57c1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_new_test = np.vstack((x_positive_test,x_negative_test))\n",
        "X_new_test[199]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anWtE_Hp_whp",
        "outputId": "91054e3a-5331-4cbe-8005-5c76fc697c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  70., 140., 254., 254., 254., 167.,\n",
              "        32.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  10., 134.,\n",
              "       249., 254., 253., 253., 253., 253.,  60.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,  11.,  78., 194., 253., 253., 254., 253., 231., 224.,\n",
              "       253.,  60.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  12., 165., 253., 253.,\n",
              "       253., 253., 200.,  84.,  32.,  17.,  96.,  55.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,  12., 165., 253., 253., 253., 198.,  77.,   0.,   0.,   0.,\n",
              "        41., 107., 233.,  34.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  49., 253., 253., 153.,\n",
              "        60.,   6.,   0.,   0.,  42.,  73., 194., 253., 253.,  48.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,  49., 253., 253., 117.,  85.,  85., 143., 207., 233.,\n",
              "       253., 253., 253., 229.,  32.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30., 226., 253.,\n",
              "       253., 253., 253., 253., 254., 198., 217., 253., 253.,  77.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,  46., 228., 253., 253., 154.,  87.,  24.,\n",
              "        10., 157., 253., 146.,   6.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         9.,  11.,  11.,   1.,   0.,   0.,  24., 249., 230.,  38.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "        19., 254., 254., 200.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0., 145., 253., 250.,  76.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "        76., 254., 250., 128.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  45., 204., 254., 229.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "       230., 253., 252., 117.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  75., 248., 253., 207.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  29.,\n",
              "       207., 253., 253.,  23.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0., 131., 253., 253., 149.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  32.,\n",
              "       226., 253., 253.,  22.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  97., 173., 253.,  92.,   1.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_label=np.ones(x_positive.shape[0])\n",
        "x_min_one_label=np.ones(x_negative.shape[0])*-1\n",
        "y_new=np.hstack((x_one_label,x_min_one_label))"
      ],
      "metadata": {
        "id": "Pm83Jh630qq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_label_test=np.ones(x_positive_test.shape[0])\n",
        "x_min_one_label_test=np.ones(x_negative_test.shape[0])*-1\n",
        "y_new_test=np.hstack((x_one_label_test,x_min_one_label_test))"
      ],
      "metadata": {
        "id": "MQLvuDsi_9kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_new.shape)\n",
        "y_new.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wBkwOpt1mx_",
        "outputId": "b3e8cdab-85f7-4ece-be0e-0e5553e108c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1992, 784)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1992,)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_new_test.shape)\n",
        "y_new_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3e_ZQ_ZAvcq",
        "outputId": "467897ac-437c-4a8e-beeb-d8f6e5248c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(409, 784)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(409,)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle(X_new, y_new, random_state=1729)\n",
        "#shuffle(y_new,random_state=1729)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U49285Zi1-c-",
        "outputId": "332ea383-d312-423c-ea4a-3fa393992f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
              " array([ 1., -1., -1., ..., -1., -1., -1.])]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle(X_new_test, y_new_test, random_state=1729)\n",
        "#shuffle(y_new_test,random_state=1729)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ajuuT2gA1-X",
        "outputId": "d00f3c2a-a764-4335-b98f-da3e1f7e68d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
              " array([-1., -1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,\n",
              "         1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1., -1., -1., -1.,\n",
              "        -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1., -1.,  1., -1.,\n",
              "         1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,\n",
              "         1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.,\n",
              "        -1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1.,  1.,\n",
              "        -1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1.,\n",
              "         1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,\n",
              "         1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,\n",
              "         1., -1., -1., -1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.,  1.,\n",
              "        -1., -1., -1.,  1.,  1., -1., -1., -1., -1.,  1., -1., -1.,  1.,\n",
              "         1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,\n",
              "         1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.,  1., -1.,\n",
              "        -1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
              "         1., -1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
              "        -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
              "         1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1.,  1.,\n",
              "         1., -1.,  1., -1.,  1., -1., -1., -1.,  1., -1., -1.,  1.,  1.,\n",
              "        -1.,  1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,\n",
              "         1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1., -1.,\n",
              "         1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
              "         1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
              "        -1.,  1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
              "        -1., -1., -1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1., -1.,\n",
              "         1.,  1.,  1.,  1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,\n",
              "         1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
              "        -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1.,\n",
              "         1.,  1., -1., -1., -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,\n",
              "         1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1.,\n",
              "        -1., -1.,  1., -1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
              "        -1., -1.,  1., -1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,  1.,\n",
              "        -1.,  1.,  1., -1.,  1.,  1.])]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron_model=Perceptron(random_state=1729, eta0=1, max_iter=1, shuffle=False, fit_intercept=True, alpha=0, warm_start=True, penalty=None)"
      ],
      "metadata": {
        "id": "jt3eFGXZ2qRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron_model.fit(X_new,y_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "KHe_DmiO--Eg",
        "outputId": "dbbdf1b6-012f-4f24-96c3-1b0a98207eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Perceptron(alpha=0, eta0=1, max_iter=1, random_state=1729, shuffle=False,\n",
              "           warm_start=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron(alpha=0, eta0=1, max_iter=1, random_state=1729, shuffle=False,\n",
              "           warm_start=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron(alpha=0, eta0=1, max_iter=1, random_state=1729, shuffle=False,\n",
              "           warm_start=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  perceptron_model.fit(X_new, y_new)\n",
        "  print(perceptron_model.intercept_)\n",
        "print(perceptron_model.coef_[0][68])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koo2ajIhlFD9",
        "outputId": "44028358-63bd-401e-bd30-bf9256ce3a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2.]\n",
            "[-3.]\n",
            "[-3.]\n",
            "[-3.]\n",
            "[-3.]\n",
            "[-4.]\n",
            "[-4.]\n",
            "[-5.]\n",
            "[-5.]\n",
            "[-5.]\n",
            "0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GA"
      ],
      "metadata": {
        "id": "u_zMkQoc3oSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X[:10000]\n",
        "y_train = y[:10000]\n",
        "X_test = X[10000:12000]\n",
        "y_test = y[10000:12000]"
      ],
      "metadata": {
        "id": "QXMiRKvJ3ss2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_5_indices = np.where(y_train=='5')\n",
        "X_3_indices = np.where(y_train=='3')\n",
        "X_5_indices_t = np.where(y_test=='5')\n",
        "X_3_indices_t = np.where(y_test=='3')"
      ],
      "metadata": {
        "id": "nSEu3nri7fYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_5 = X_train[X_5_indices]\n",
        "X_3 = X_train[X_3_indices]\n",
        "X_5_t = X_test[X_5_indices_t]\n",
        "X_3_t = X_test[X_3_indices_t]"
      ],
      "metadata": {
        "id": "a2hHVzR28yEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_5.shape)\n",
        "print(X_3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h04V-HrTzmPU",
        "outputId": "d4f22d4e-eab7-47d7-d9d2-68369f0e9a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(863, 784)\n",
            "(1032, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_final = np.row_stack((X_5, X_3))\n",
        "y_final = np.concatenate(([1] * X_5.shape[0], [-1] * X_3.shape[0]))"
      ],
      "metadata": {
        "id": "H0dTsfvEgoZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_final.shape)\n",
        "y_final.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkUgyg24zHlo",
        "outputId": "90760128-c64e-4771-8c45-f207f1344c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1895, 784)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1895,)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_final_t = np.row_stack((X_5_t, X_3_t))\n",
        "y_final_t = np.concatenate(([1] * X_5_t.shape[0], [-1] * X_3_t.shape[0]))"
      ],
      "metadata": {
        "id": "RuhwVibgzRR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_final_t.shape)\n",
        "y_final_t.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByEaNgyszc7b",
        "outputId": "debd29a7-2db4-41ba-a4a1-d1098b3322c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(381, 784)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(381,)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per_mod=Perceptron(random_state=42, eta0=1, max_iter=1, penalty=None, alpha=0, shuffle=True, fit_intercept=True, warm_start=True)"
      ],
      "metadata": {
        "id": "s6pChTPFzyE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "  per_mod.fit(X_final,y_final)\n",
        "print(perceptron_model.intercept_)\n",
        "print(perceptron_model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxeMjTFJ0Wqa",
        "outputId": "c7e78329-15d7-4add-8215-6edb37f69205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-5.]\n",
            "[[ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  1.550e+02\n",
            "   3.290e+02  6.170e+02  4.620e+02  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  1.200e+01  3.400e+02  3.360e+02\n",
            "   3.210e+02  5.800e+02  4.530e+02  7.380e+02  8.310e+02  8.510e+02\n",
            "   8.910e+02  9.450e+02  7.200e+02  8.200e+01  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  1.700e+02  7.330e+02  5.100e+02  1.760e+02  5.050e+02\n",
            "   1.222e+03  1.578e+03  1.377e+03  1.260e+03  1.127e+03  1.235e+03\n",
            "   9.300e+02  3.290e+02  2.160e+02  2.700e+01  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  2.400e+01  3.590e+02\n",
            "   6.150e+02 -5.200e+01 -1.880e+02  3.460e+02  1.960e+02  2.370e+02\n",
            "   2.220e+02  3.540e+02  7.300e+02  8.270e+02  6.640e+02  2.700e+02\n",
            "   2.530e+02  1.360e+02  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  1.080e+02  5.190e+02 -3.600e+01  1.790e+02\n",
            "  -4.020e+02 -3.620e+02 -1.300e+02  1.270e+02 -4.510e+02 -1.061e+03\n",
            "  -4.090e+02 -7.450e+02 -4.120e+02  2.520e+02  2.360e+02  2.800e+01\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   8.800e+01  1.310e+02 -3.640e+02 -9.030e+02 -9.440e+02 -4.790e+02\n",
            "  -8.030e+02 -1.523e+03 -1.613e+03 -2.017e+03 -1.313e+03 -1.343e+03\n",
            "  -1.149e+03  2.760e+02  5.570e+02  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00 -9.000e+00  8.700e+01 -1.800e+01\n",
            "  -1.077e+03 -1.178e+03 -5.670e+02  2.300e+02 -9.490e+02 -1.262e+03\n",
            "  -1.233e+03 -1.530e+03 -2.149e+03 -2.330e+03 -1.665e+03  9.200e+01\n",
            "   7.550e+02  3.030e+02  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "  -1.000e+01 -1.240e+02  1.490e+02 -6.970e+02 -1.313e+03 -8.700e+02\n",
            "   7.010e+02 -6.200e+01 -3.300e+02  1.340e+02  4.540e+02 -8.560e+02\n",
            "  -2.076e+03 -1.585e+03 -1.587e+03 -1.085e+03  2.520e+02  1.330e+02\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00 -2.900e+01 -1.830e+02 -1.710e+02\n",
            "  -3.930e+02 -7.190e+02 -1.214e+03 -6.780e+02 -1.620e+02 -2.000e+00\n",
            "  -5.400e+01  1.600e+02 -2.400e+01 -1.172e+03 -1.551e+03 -1.526e+03\n",
            "  -1.063e+03 -4.900e+02  5.550e+02  2.710e+02  3.000e+01  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00 -3.190e+02 -1.810e+02 -5.840e+02 -6.750e+02 -6.470e+02\n",
            "  -8.070e+02 -6.000e+02 -3.110e+02 -4.030e+02 -1.680e+02 -4.800e+02\n",
            "  -1.071e+03 -1.229e+03 -7.410e+02 -5.080e+02 -1.220e+03  2.210e+02\n",
            "   6.370e+02  2.870e+02  3.100e+02  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00 -2.160e+02 -3.930e+02\n",
            "  -4.330e+02 -8.860e+02 -6.260e+02 -3.780e+02  1.900e+02  2.940e+02\n",
            "  -4.100e+01 -2.700e+01 -7.000e+00 -5.980e+02 -1.432e+03 -8.010e+02\n",
            "  -1.580e+02 -8.780e+02 -5.520e+02  6.860e+02  6.870e+02  2.400e+01\n",
            "   5.060e+02  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00 -7.000e+01 -5.140e+02 -3.350e+02 -4.550e+02 -9.860e+02\n",
            "  -4.840e+02  4.190e+02  7.530e+02  1.143e+03  9.370e+02  9.540e+02\n",
            "   2.060e+02 -1.400e+02 -4.290e+02 -3.980e+02 -2.530e+02 -8.750e+02\n",
            "  -1.810e+02  1.250e+03  1.008e+03  3.920e+02  3.880e+02  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00 -1.270e+02\n",
            "  -6.600e+02 -3.080e+02 -6.760e+02 -6.380e+02 -4.750e+02  3.560e+02\n",
            "   7.210e+02  8.630e+02  3.440e+02 -8.700e+01  9.800e+01  4.010e+02\n",
            "  -8.340e+02 -7.120e+02 -3.470e+02 -1.461e+03 -1.440e+02  1.421e+03\n",
            "   9.490e+02  4.980e+02  1.320e+02  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00 -1.300e+01 -6.000e+02 -3.070e+02\n",
            "  -3.790e+02 -2.260e+02  9.000e+00 -2.910e+02 -2.270e+02  1.900e+02\n",
            "  -4.550e+02 -9.600e+01  1.230e+02  1.450e+02 -9.650e+02 -1.340e+02\n",
            "  -1.860e+02 -4.010e+02 -1.290e+02  1.052e+03  1.021e+03  7.520e+02\n",
            "   7.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00 -2.000e+00 -4.310e+02  5.300e+01 -1.680e+02  1.170e+02\n",
            "   4.460e+02  3.620e+02  3.370e+02  5.020e+02 -1.980e+02  2.590e+02\n",
            "   1.140e+02 -1.200e+02 -8.210e+02 -3.620e+02 -9.600e+01  8.860e+02\n",
            "  -1.420e+02  5.700e+02  1.221e+03  4.570e+02  7.000e+01  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "  -1.330e+02 -7.000e+01 -1.400e+02  5.370e+02  8.360e+02  6.890e+02\n",
            "   1.424e+03  1.836e+03  9.410e+02  8.400e+01 -5.730e+02 -8.880e+02\n",
            "  -2.760e+02 -4.550e+02  1.700e+02  5.680e+02  4.770e+02  8.200e+02\n",
            "   7.830e+02  2.700e+02  1.900e+01  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00 -4.000e+00  1.600e+01\n",
            "  -5.110e+02  4.570e+02  1.053e+03  9.500e+02  1.608e+03  1.180e+03\n",
            "   6.620e+02  4.990e+02  3.880e+02  5.900e+02  4.250e+02 -4.770e+02\n",
            "   1.500e+02  5.900e+01  2.600e+02  6.190e+02  4.170e+02  1.400e+02\n",
            "  -3.700e+01  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  1.000e+00  8.600e+01 -1.660e+02  2.380e+02\n",
            "   5.280e+02  9.340e+02  1.111e+03  1.098e+03  1.396e+03  1.269e+03\n",
            "   1.146e+03  7.560e+02  5.410e+02 -3.900e+02 -7.770e+02 -5.590e+02\n",
            "   2.560e+02  7.720e+02  3.130e+02 -3.000e+01 -1.540e+02  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  1.360e+02 -1.870e+02  3.900e+02  9.550e+02  3.980e+02\n",
            "   9.440e+02  1.342e+03  1.481e+03  1.394e+03  9.600e+02  7.790e+02\n",
            "   5.120e+02 -4.170e+02 -5.800e+02  4.150e+02  6.960e+02  3.800e+02\n",
            "  -1.110e+02 -2.160e+02 -1.960e+02  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  2.900e+01\n",
            "   4.300e+01  1.430e+02  5.760e+02  4.570e+02  3.380e+02  1.428e+03\n",
            "   1.337e+03  1.169e+03  8.320e+02  1.300e+02  1.000e+01 -5.360e+02\n",
            "   6.310e+02  1.037e+03  3.950e+02 -3.910e+02 -4.160e+02 -2.520e+02\n",
            "  -1.660e+02  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00 -6.500e+01  6.800e+01\n",
            "   2.100e+01  1.900e+02  1.940e+02 -3.940e+02  4.200e+01  1.106e+03\n",
            "   9.070e+02  5.800e+02 -4.170e+02 -9.050e+02  3.800e+01  1.390e+02\n",
            "  -5.770e+02 -7.690e+02 -5.630e+02 -2.250e+02 -1.600e+02  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00 -1.800e+01 -2.720e+02 -5.970e+02 -5.690e+02 -1.320e+02\n",
            "  -3.470e+02 -6.640e+02 -6.400e+02 -3.680e+02 -2.530e+02 -1.020e+03\n",
            "  -1.283e+03 -5.550e+02  8.600e+01 -5.600e+02 -6.420e+02 -9.410e+02\n",
            "  -5.880e+02 -1.050e+02 -2.200e+01  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00 -6.800e+01\n",
            "  -5.140e+02 -5.990e+02 -2.080e+02 -1.890e+02 -7.130e+02 -6.290e+02\n",
            "  -4.950e+02 -4.640e+02 -4.590e+02 -9.960e+02 -1.909e+03 -9.880e+02\n",
            "  -4.900e+02 -6.870e+02 -6.980e+02 -8.300e+02 -7.590e+02 -4.110e+02\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00 -1.960e+02 -5.040e+02 -4.570e+02\n",
            "  -2.750e+02 -3.030e+02 -6.360e+02 -2.680e+02 -3.030e+02 -2.940e+02\n",
            "  -8.100e+01 -3.680e+02 -8.710e+02 -8.180e+02 -1.410e+02 -3.390e+02\n",
            "  -4.150e+02 -4.600e+01  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00 -5.400e+01 -2.280e+02 -1.290e+02\n",
            "  -2.800e+01  0.000e+00 -1.790e+02 -9.000e+00  0.000e+00 -2.800e+01\n",
            "  -2.940e+02 -5.040e+02 -8.400e+01  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
            "   0.000e+00  0.000e+00  0.000e+00  0.000e+00]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per_mod.score(X_final, y_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzSHVnVm1IBQ",
        "outputId": "0d70911c-57ea-46d6-950b-189f0165971b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9857519788918205"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "per_mod.score(X_final_t, y_final_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpnnA4ko3yEF",
        "outputId": "2e95e819-bbef-4807-b686-e33c19368fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.958005249343832"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_test = per_mod.predict(X_final_t)"
      ],
      "metadata": {
        "id": "GOHAqm-x37lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_display = ConfusionMatrixDisplay.from_predictions(y_final_t, y_hat_test, values_format='.5g')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XlQhZR9a4TBL",
        "outputId": "8afa1e94-805e-493a-9583-f4908e00efa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}